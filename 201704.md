# 2017-04-19
 - 使用crawler parse plugin生成饿了么解析job。
 > + 了解spark里，RDD的API里所引用的所有对象，都必须是可序列化的，因为RDD分布在多台机器是，代码和所引用的对象会序列化，然后复制到多台机器，所以凡是被引用的数据，都必须是可序列化的。插件使用到了flatmap参数没有序列化，所以抽取成静态全局变量来解决
 - 继续看thinking in java, 复习java基础知识
 - 根据公司Crane的wiki 衍生学习了java nio, netty 等知识

# 2017-04-20
 - 学习java的日志系统，学习log4j的结构和使用,slf4j作为框架接口
 - 继续复习java语法相关的知识
 - 开会学习了自动化抽取和信息对齐的一些相关知识

# 2017-04-21
 - 学习了docker相关的知识和历史发展
 - 继续看hulk相关的技术文档
 
# 2017-04-24
 - 建立新的git仓库，整合zbb_crawler和zbb_commons
 - 开始对新建的爬虫仓库进行整理
 - 参加公司“聊聊业务开发”培训

# 2017-04-25
 - 整理技术分享ppt
 - 更新pycrawler仓库，整理了项目结构，验证可用
 - 在pycrawler上建立分支refactor，尝试整理代码
 - 在pycrawler上建立分支newrm，尝试改成新的存储方式
 
# 2017-04-26
 - 整理测试了wheel打包pycrawler。**测试可用，不过再最终调用接口上应该要修改**
 - 根据新的爬虫对接数据规范改动了close tag和pid文件相关的代码
 - 做了技术分享
 - 看了kafka相关的一些简单介绍

# 2017-04-27
 - 依赖安装的pycrawler运行爬虫程序测试 **测试可用，之后解决依赖问题**
 - 根据新的数据存储规范改动代码 **大部分改完测完，不过db时间戳命名牵扯比较大，还需修改**

# 2017-04-28
 - 根据新的存储标准修改代码存储逻辑
 - 整理pycrawler打包依赖
 - 联调docker部署
